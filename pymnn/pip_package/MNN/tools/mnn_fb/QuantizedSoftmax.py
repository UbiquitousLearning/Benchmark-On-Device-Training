# automatically generated by the FlatBuffers compiler, do not modify

# namespace: MNN

import flatbuffers

class QuantizedSoftmax(object):
    __slots__ = ['_tab']

    @classmethod
    def GetRootAsQuantizedSoftmax(cls, buf, offset):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
        x = QuantizedSoftmax()
        x.Init(buf, n + offset)
        return x

    # QuantizedSoftmax
    def Init(self, buf, pos):
        self._tab = flatbuffers.table.Table(buf, pos)

    # QuantizedSoftmax
    def Beta(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Float32Flags, o + self._tab.Pos)
        return 0.0

    # QuantizedSoftmax
    def InputScale(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            return self._tab.Get(flatbuffers.number_types.Float32Flags, o + self._tab.Pos)
        return 0.0

def QuantizedSoftmaxStart(builder): builder.StartObject(2)
def QuantizedSoftmaxAddBeta(builder, beta): builder.PrependFloat32Slot(0, beta, 0.0)
def QuantizedSoftmaxAddInputScale(builder, inputScale): builder.PrependFloat32Slot(1, inputScale, 0.0)
def QuantizedSoftmaxEnd(builder): return builder.EndObject()
