# LLM on Edge
#ç§‘ç ”/ç§‘ç ”æ–¹å‘/Large-language-model

## åŸºç¡€çŸ¥è¯†
- Transformerï¼šRead Attention and BERT papers.
- Attentionï¼š[Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- BERTï¼š[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- GPTï¼š[GPT](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf), [GPT2](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf), [GPT3](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf), [InstructGPT: GPT3.5](https://arxiv.org/pdf/2203.02155.pdf?fbclid=IwAR2nZdBpdZZzvxpwI6H_bRmP4RwGOyzke9Ud63lWBe1YlyI_1BRAFhnUMUg)
- Survey: [Foundation model](https://arxiv.org/pdf/2108.07258.pdf?utm_source=morning_brew),  [From cloud to edge](https://dl.acm.org/doi/abs/10.1145/3487552.3487815)


---
## å‚è€ƒèµ„æ–™
1. [OpenAIçš„äººå†™çš„å¯¹å¤§æ¨¡å‹çš„ä¼˜åŒ–æ•´ç†](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
2. ææ²è®²AI: [Transformer](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0), [Swin Transformer](https://www.bilibili.com/video/BV13L4y1475U/?spm_id_from=333.999.0.0),   [GPTï¼ŒGPT-2ï¼ŒGPT-3 è®ºæ–‡ç²¾è¯»](https://www.bilibili.com/video/BV1AF411b7xQ/) ,[InstructGPT è®ºæ–‡ç²¾è¯»](https://www.bilibili.com/video/BV1hd4y187CR/?spm_id_from=333.999.0.0)
3. ChatGPT: [Notion â€“ The all-in-one workspace for your notes, tasks, wikis, and databases.](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756)
4. Transformer code: [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
5. ä¸»æµçš„Tranformer communityï¼š [ğŸ¤— Transformers](https://huggingface.co/docs/transformers/index) 
