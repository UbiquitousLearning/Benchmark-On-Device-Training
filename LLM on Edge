# LLM on Edge
#科研/科研方向/Large-language-model

## 基础知识
- Transformer：Read Attention and BERT papers.
- Attention：[Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- BERT：[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- GPT：[GPT](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf), [GPT2](https://life-extension.github.io/2020/05/27/GPT%E6%8A%80%E6%9C%AF%E5%88%9D%E6%8E%A2/language-models.pdf), [GPT3](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf), [InstructGPT: GPT3.5](https://arxiv.org/pdf/2203.02155.pdf?fbclid=IwAR2nZdBpdZZzvxpwI6H_bRmP4RwGOyzke9Ud63lWBe1YlyI_1BRAFhnUMUg)
- Survey: [Foundation model](https://arxiv.org/pdf/2108.07258.pdf?utm_source=morning_brew),  [From cloud to edge](https://dl.acm.org/doi/abs/10.1145/3487552.3487815)


---
## 参考资料
1. [OpenAI的人写的对大模型的优化整理](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
2. 李沐讲AI: [Transformer](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0), [Swin Transformer](https://www.bilibili.com/video/BV13L4y1475U/?spm_id_from=333.999.0.0),   [GPT，GPT-2，GPT-3 论文精读](https://www.bilibili.com/video/BV1AF411b7xQ/) ,[InstructGPT 论文精读](https://www.bilibili.com/video/BV1hd4y187CR/?spm_id_from=333.999.0.0)
3. ChatGPT: [Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756)
4. Transformer code: [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
5. 主流的Tranformer community： [🤗 Transformers](https://huggingface.co/docs/transformers/index) 
