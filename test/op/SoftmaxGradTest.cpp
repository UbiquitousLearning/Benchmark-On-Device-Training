//
//  SoftmaxGradTest.cpp
//  MNNTests
//
//  Created by MNN on 2019/10/16.
//  Copyright Â© 2018, Alibaba Group Holding Limited
//

#include <MNN/MNNForwardType.h>
#include <MNN/expr/Expr.hpp>
#include <MNN/expr/ExprCreator.hpp>
#include <MNN/expr/Optimizer.hpp>
#include <string>
#include <vector>
#include "MNNTestSuite.h"
#include "MNN_generated.h"
#include "TestUtils.h"

using namespace MNN::Express;

static VARP _SoftmaxGrad(VARP originOutput, VARP outputGrad, int axis) {
    using namespace MNN;
    std::unique_ptr<OpT> softmax(new OpT);
    softmax->type                = OpType_SoftmaxGrad;
    softmax->main.type           = OpParameter_Axis;
    softmax->main.value          = new AxisT;
    softmax->main.AsAxis()->axis = axis;
    return Variable::create(Expr::create(std::move(softmax), {originOutput, outputGrad}));
}

class SoftmaxGradTest : public MNNTestCase {
public:
    virtual ~SoftmaxGradTest() = default;

protected:
    bool testOnBackend(MNNForwardType type, const std::string &deviceName) {
        const int batch = 4, channel = 4, size = batch * channel;
        float originOutputData[batch][channel] = {
            {0.2, 0.23, 0.3, 0.27}, {0.18, 0.33, 0.16, 0.33}, {0.15, 0.18, 0.35, 0.32}, {0.29, 0.18, 0.22, 0.31}};
        float outputGradData[batch][channel] = {{1., 2., 3., 4.}, {2., 3., 4., 1.}, {3., 4., 1., 2.}, {4., 1., 2., 3.}};
        float expectGrad[batch][channel];
        for (int b = 0; b < batch; ++b) {
            float sum = 0;
            for (int c = 0; c < channel; ++c) {
                sum += originOutputData[b][c] * outputGradData[b][c];
            }
            for (int c = 0; c < channel; ++c) {
                expectGrad[b][c] = originOutputData[b][c] * (outputGradData[b][c] - sum);
            }
        }

        auto output            = _Input({batch, channel}, NCHW, halide_type_of<float>());
        auto outputGrad        = _Input({batch, channel}, NCHW, halide_type_of<float>());
        auto outputConvert     = _Convert(output, NC4HW4);
        auto outputGradConvert = _Convert(outputGrad, NC4HW4);
        auto softmaxGrad       = _Convert(_SoftmaxGrad(outputConvert, outputGradConvert, 1), NCHW);

        if (type != MNN_FORWARD_CPU) {
            Optimizer::Config config;
            config.forwardType = type;
            auto optimizer     = Optimizer::create(config);
            if (optimizer == nullptr) {
                MNN_ERROR("backend %s not support\n", deviceName.c_str());
                return false;
            }
            optimizer->onExecute({softmaxGrad});
        }

        const std::vector<int> outDim = {batch, channel};
        auto softmaxGradDim           = softmaxGrad->getInfo()->dim;
        if (!checkVector<int>(softmaxGradDim.data(), outDim.data(), 2, 0)) {
            MNN_ERROR("SoftmaxGrad(%s) shape test failed!\n", deviceName.c_str());
            return false;
        }

        ::memcpy(output->writeMap<float>(), (const float *)originOutputData, size * sizeof(float));
        ::memcpy(outputGrad->writeMap<float>(), (const float *)outputGradData, size * sizeof(float));
        auto compute = softmaxGrad->readMap<float>();
        if (!checkVectorByRelativeError<float>(compute, (const float *)expectGrad, size, 0.005)) {
            MNN_ERROR("SoftmaxGrad(%s) test failed!\n", deviceName.c_str());
            return false;
        }
        return true;
    }
};

class SoftmaxGradTestOnCPU : public SoftmaxGradTest {
public:
    virtual ~SoftmaxGradTestOnCPU() = default;
    virtual bool run() {
        return testOnBackend(MNN_FORWARD_CPU, "CPU");
    }
};

MNNTestSuiteRegister(SoftmaxGradTestOnCPU, "op/SoftmaxGrad");
